{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from tools import feature_extraction, table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load_data: loads data from the given path. can work with interpolated, accel, or imu data.\n",
    "Parameters: \n",
    "path: Path to the data directory.\n",
    "Returns:\n",
    " df: A dataframe. This dataframe will be different, wheter we are passing in imu data, accel data, or interpolated/merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(file_name):\n",
    "    # the tap strap has 5 xyz accelerometers\n",
    "    # and a imu on the thumb. The \n",
    "    data = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    df = pd.DataFrame(data)\n",
    "    # Break payload into separate columns\n",
    "    if all(len(i) == 15 for i in df['payload']): # case we are loading in data from the general tap strap \n",
    "        df[['thumb_x', 'thumb_y', 'thumb_z', 'index_x', 'index_y', 'index_z', 'middle_x', 'middle_y', 'middle_z', \n",
    "            'ring_x', 'ring_y', 'ring_z', 'pinky_x', 'pinky_y', 'pinky_z']] = pd.DataFrame(df['payload'].values.tolist(), index=df.index)\n",
    "    elif all(len(i) == 6 for i in df['payload']): # case we are loading in imu data for the thumb \n",
    "        df[['thumb_imu_x', 'thumb_imu_y', 'thumb_imu_z', 'thumb_imu_pitch', 'thumb_imu_yaw', 'thumb_imu_roll',\n",
    "            ]] = pd.DataFrame(df['payload'].values.tolist(), index=df.index)\n",
    "    elif all(len(i) == 21 for i in df['payload']): # case we are loading in merged/interpolated data \n",
    "        df[['thumb_imu_x', 'thumb_imu_y', 'thumb_imu_z', 'thumb_imu_pitch', 'thumb_imu_yaw', 'thumb_imu_roll', 'thumb_x', 'thumb_y', 'thumb_z', 'index_x', 'index_y', 'index_z', 'middle_x', 'middle_y', 'middle_z', \n",
    "            'ring_x', 'ring_y', 'ring_z', 'pinky_x', 'pinky_y', 'pinky_z' ,\n",
    "            ]] = pd.DataFrame(df['payload'].values.tolist(), index=df.index)\n",
    "    else:\n",
    "        print(\"Some payloads do not have the expected length of 15 or 6.\")\n",
    "    \n",
    "    # Drop the original 'payload' column\n",
    "    df = df.drop(columns=['payload'])\n",
    "    df\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(df):\n",
    "    # List of fingers and axes\n",
    "    fingers = ['thumb', 'index', 'middle', 'ring', 'pinky']\n",
    "    axes = ['x', 'y', 'z']\n",
    "    \n",
    "    # Calculate mean and std for each finger's coordinates\n",
    "    for finger in fingers:\n",
    "        for axis in axes:\n",
    "            df[f'{finger}_{axis}_mean'] = df[f'{finger}_{axis}'].mean()\n",
    "            df[f'{finger}_{axis}_std'] = df[f'{finger}_{axis}'].std()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def plot_data(df, title):\n",
    "    # List of fingers and axes\n",
    "    fingers = ['thumb', 'index', 'middle', 'ring', 'pinky']\n",
    "    axes = ['x', 'y', 'z']\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    calculate_statistics(df)\n",
    "    \n",
    "    for finger in fingers:\n",
    "        for axis in axes:\n",
    "            plt.plot(df['timestamp'], df[f'{finger}_{axis}_mean'], label=f'{finger}_{axis}')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Mean Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# plot_data(imu_df, 'IMU Data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensive Plots: will plot the data for each gesture in a separate plot, with the x-axis being the time and the y-axis being the value of the sensor. This will help you visualize the data and get a better understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def extensive_plot_helper(df, x,y_list, folder_name=''):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    for y in y_list:\n",
    "         plt.plot(df[x], df[y], label=y)\n",
    "    # take y list and turn into a string\n",
    "    y_list = ', '.join(y_list)\n",
    "    plt.title(folder_name + \" \" + str(y_list) + \" over time\")\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Values')\n",
    "    plt.legend()\n",
    "\n",
    "def extensive_plots(accel_df,imu_df, interpolated, folder_name=''): \n",
    "\n",
    "    extensive_plot_helper(interpolated, 'timestamp', ['thumb_imu_x', 'thumb_imu_y', 'thumb_imu_z', 'thumb_imu_pitch', 'thumb_imu_yaw', 'thumb_imu_roll'], folder_name + \" interpolated\")\n",
    "    extensive_plot_helper(imu_df, 'timestamp', ['thumb_imu_x', 'thumb_imu_y', 'thumb_imu_z', 'thumb_imu_pitch', 'thumb_imu_yaw', 'thumb_imu_roll'], folder_name + \" imu\")\n",
    "    extensive_plot_helper(accel_df, 'timestamp', ['thumb_x', 'thumb_y', 'thumb_z'], folder_name + \" accel\")\n",
    "    # Do the same for the rest of the fingers\n",
    "    extensive_plot_helper(accel_df, 'timestamp', ['index_x', 'index_y', 'index_z'], folder_name + \" accel\")\n",
    "    # Similarly for middle, ring, and pinky fingers...\n",
    "    extensive_plot_helper(accel_df, 'timestamp', ['middle_x', 'middle_y', 'middle_z'], folder_name + \" accel\")\n",
    "    extensive_plot_helper(accel_df, 'timestamp', ['pinky_x', 'pinky_y', 'pinky_z'], folder_name + \" accel\")\n",
    "    extensive_plot_helper(accel_df, 'timestamp', ['pinky_x', 'pinky_y', 'pinky_z'], folder_name + \" accel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMU and accelerometer data\n",
    "# print current directory\n",
    "print(sys.path[0])\n",
    "# interpolated test\n",
    "\n",
    "interpolated_df = load_data('./training_data/data/Turn2/merged_data.json')\n",
    "imu_df = load_data('./training_data/data/Turn2/imu_data.json')\n",
    "test_accel_df = load_data('./training_data/data/Turn2/accel_data.json')\n",
    "extensive_plots(test_accel_df,imu_df, interpolated_df, folder_name='Turn2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_3d: will plot the data in 3d. This will help you visualize the data and get a better understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_scatter(df, finger ):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    # make thumb_x red, thumb_y green, thumb_z blue\n",
    "    ax.scatter(df[f'{finger}_x'], df[f'{finger}_y'], df[f'{finger}_z'])\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    plt.title(\"f'{finger} IMU Values\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_3d(df):\n",
    "    plot_scatter(df, 'thumb')\n",
    "    plot_scatter(df, 'index')\n",
    "    plot_scatter(df, 'middle')\n",
    "    plot_scatter(df, 'ring')\n",
    "    plot_scatter(df, 'pinky')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d(interpolated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(accel_df):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Thumb\n",
    "    ax.scatter(accel_df['thumb_x'], accel_df['thumb_y'], accel_df['thumb_z'], c='b', label='Thumb')\n",
    "\n",
    "    # Index\n",
    "    ax.scatter(accel_df['index_x'], accel_df['index_y'], accel_df['index_z'], c='g', label='Index')\n",
    "\n",
    "    # Middle\n",
    "    ax.scatter(accel_df['middle_x'], accel_df['middle_y'], accel_df['middle_z'], c='r', label='Middle')\n",
    "\n",
    "    # Ring\n",
    "    ax.scatter(accel_df['ring_x'], accel_df['ring_y'], accel_df['ring_z'], c='c', label='Ring')\n",
    "\n",
    "    # Pinky\n",
    "    ax.scatter(accel_df['pinky_x'], accel_df['pinky_y'], accel_df['pinky_z'], c='m', label='Pinky')\n",
    "\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    plt.legend()\n",
    "    plt.title('Accel values for all fingers')\n",
    "    plt.show()\n",
    "\n",
    "plot_all(test_accel_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the same calculation for other fingers\n",
    "# Average acceleration magnitude pe\n",
    "# avg_accel_mag = ((imu_df[['thumb_x', 'thumb_y', 'thumb_z']]**2).sum(axis=1)**0.5).mean()\n",
    "# print all of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of fingers\n",
    "fingers = ['thumb', 'index', 'middle', 'ring', 'pinky']\n",
    "\n",
    "# List to hold features\n",
    "features = ['thumb_x', 'thumb_y', 'thumb_z', 'index_x', 'index_y', 'index_z', \n",
    "            'middle_x', 'middle_y', 'middle_z', 'ring_x', 'ring_y', 'ring_z', \n",
    "            'pinky_x', 'pinky_y', 'pinky_z']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_features = feature_extraction(accel_df)\n",
    "feature_extraction(test_accel_df, use_label=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"./training_data/data/Still2\"))\n",
    "# filter the list to contain folders only with the word \"still\" in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to do the feature extraction with the interpolated data \n",
    "dir_list = os.listdir(\"./training_data/data_2\")\n",
    "\n",
    "num_still_folders = len([i for i in dir_list if \"still\" in i])\n",
    "num_turn_folders = len([i for i in dir_list if \"turn\" in i])\n",
    "num_lever_folders = len([i for i in dir_list if \"lever\" in i])\n",
    "gesture_folders = [('lever', num_lever_folders), ('turn', num_turn_folders), ('still', num_still_folders) ] # will refcator this into a simpler loop\n",
    "print(\"Gesture Folders: \", gesture_folders)\n",
    "acc = pd.DataFrame()\n",
    "count = 0\n",
    "# '../../data/Still2/imu_data.json'\n",
    "for gesture_name,number_items in gesture_folders:\n",
    "    print('looking at ',gesture_name, \"with \", number_items, \"number of folders\" )\n",
    "    for  i in range(number_items):\n",
    "        # load the data\n",
    "        file_name = str(f'training_data/data_2/{gesture_name}{i}/merged_data.json')\n",
    "        ndf = load_data(file_name)\n",
    "        features = feature_extraction(ndf, use_label = True, interpolated=True)\n",
    "        count += 1\n",
    "        acc = pd.concat([acc, features], ignore_index=True)\n",
    "\n",
    "\n",
    "# print(\"total appendage count\", count)\n",
    "table(acc)\n",
    "final_data = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Our feature extraction, we will be using the following features:\n",
    "1. Mean\n",
    "2. Standard Deviation\n",
    "3. Median\n",
    "4. Max\n",
    "5. Min\n",
    "6. Range\n",
    "7. Interquartile Range\n",
    "8. Skewness\n",
    "9. Kurtosis\n",
    "10. Zero Crossing Rate\n",
    "11. Mean Absolute Deviation\n",
    "12. Root Mean Square\n",
    "\n",
    "### Non interpolated data feature extraction: This function will extract the features from the non interpolated data. It will return a dataframe with the features and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X = extract_features(X)\n",
    "# iterate through the \"still\" folders, perform the feature extraction on each one, and add it to the dataframe\n",
    "# get the number of folders in the still directory\n",
    "\n",
    "# dir_list = os.listdir(\"./training_data/data\")\n",
    "\n",
    "# num_still_folders = len([i for i in dir_list if \"Still\" in i])\n",
    "# num_turn_folders = len([i for i in dir_list if \"Turn\" in i])\n",
    "# num_lever_folders = len([i for i in dir_list if \"lever\" in i])\n",
    "# gesture_folders = [('Lever', num_lever_folders), ('Turn', num_turn_folders), ('Still', num_still_folders) ] # will refcator this into a simpler loop\n",
    "# print(\"Gesture Folders: \", gesture_folders)\n",
    "# acc = pd.DataFrame()\n",
    "# count = 0\n",
    "# # '../../data/Still2/imu_data.json'\n",
    "# for gesture_name,number_items in gesture_folders:\n",
    "#     print('looking at ',gesture_name, \"with \", number_items, \"number of folders\" )\n",
    "#     for  i in range(number_items):\n",
    "#         # load the data\n",
    "#         file_name = str(f'training_data/data/{gesture_name}{i}/accel_data.json')\n",
    "#         ndf = load_data(file_name)\n",
    "#         features = feature_extraction(ndf, use_label = True)\n",
    "#         count += 1\n",
    "#         acc = pd.concat([acc, features], ignore_index=True)\n",
    "\n",
    "\n",
    "# # print(\"total appendage count\", count)\n",
    "# table(acc)\n",
    "# final_data = acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(final_data)\n",
    "# table(final_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Training: This function will train the model on the training data and return the model. This function will also print out the accuracy of the model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "classifiers = [\n",
    "    RandomForestClassifier(n_estimators=100),\n",
    "    # SVC(kernel=\"linear\", degree=3, C=1),\n",
    "    # SVC(kernel=\"rbf\", degree=3, C=1),\n",
    "# \n",
    "    # KNeighborsClassifier(),\n",
    "    # LogisticRegression()\n",
    "]\n",
    "# implement histogram of oriented\n",
    "# classifiers = [\n",
    "    # RandomForestClassifier(n_estimators=500), # n_estimators is the number of trees in the forest.\n",
    "    # SVC(kernel=\"linear\"),# C represents the penalty parameter of the error term. It controls the trade off between smooth decision boundary and classifying the training points correctly.\n",
    "    # KNeighborsClassifier(n_neighbors=10), # n_neighbors is the number of neighbors to use by default for kneighbors queries.\n",
    "    # LogisticRegression( solver='lbfgs', multi_class='multinomial') # solver is the algorithm to use in the optimization problem, and multi_class is the strategy to use for multiclass problems.\n",
    "# ]\n",
    "\n",
    "final_data = final_data.sample(frac=1).reset_index(drop=True) # shuffle the data, and reset the index, drop=True means we don't want to keep the old index\n",
    "X = final_data.drop('label', axis=1)  # Use all columns except 'label' as features\n",
    "# print(final_data)\n",
    "\n",
    "# obtain only the label column\n",
    "y = final_data['label'] # Use 'label' as the target\n",
    "y = pd.DataFrame(y)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # random state is the seed used by the random number generator\n",
    "\n",
    "\n",
    "trained_models = []\n",
    "\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(type(classifier).__name__)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(report)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Still', 'Turn', 'Lever'], yticklabels=['Still', 'Turn', 'Lever'])\n",
    "    plt.xlabel(\"Predicted labels\")\n",
    "    plt.ylabel(\"True labels\")\n",
    "    plt.title(f\"Confusion Matrix - {type(classifier).__name__}\")\n",
    "    plt.show()\n",
    "\n",
    "    trained_models.append(classifier)  # Save the trained model for later use\n",
    "    feature_names = list(X.columns)\n",
    "    if isinstance(classifier, RandomForestClassifier):\n",
    "        feature_importances = classifier.feature_importances_\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': feature_importances\n",
    "        })\n",
    "        importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "        print(\"Feature Importance Ranking:\")\n",
    "        print(importance_df)\n",
    "        # put the imporance df into a file named importance.txt\n",
    "        importance_df.to_csv(f'importance_{type(classifier).__name__}.txt', index=False)\n",
    "        # take the top 10 features and plot them\n",
    "        importance_df = importance_df.head(10)\n",
    "        # put the importance df in a bar chart for visualization\n",
    "        importance_df.plot.bar(x='Feature', y='Importance',  rot=60, title=f'Feature Importance - {type(classifier).__name__}', figsize=(10, 5))\n",
    "        plt.title(f'Feature Importance - {type(classifier).__name__}')\n",
    "        plt.show()\n",
    "        \n",
    "        print()\n",
    "\n",
    "# save the files using sklearn \n",
    "\n",
    "# Save the trained models\n",
    "for i, model in enumerate(trained_models):\n",
    "    # joblib.dump(model, f'models/{type(model).__name__}_{i}.pkl')\n",
    "    joblib.dump(model, f'models/{type(model).__name__}.pkl')\n",
    "\n",
    "# make a table to show the accuracy of each model\n",
    "for i, model in enumerate(trained_models):\n",
    "    print(type(model).__name__, \"accuracy: \", accuracy_score(y_test, model.predict(X_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is an attempt to use the interpolated sensor data, but now using tensorflow and a neural network. This is a work in progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload model and do inference in real time \n",
    "# load the model from disk  \n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume X_train, X_test, y_train, y_test are your training and test sets\n",
    "\n",
    "# Standardize the input features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"X_train_scaled.shape\", X_train_scaled.shape)\n",
    "# print number of features going into neural network\n",
    "print(\"X_train_scaled.shape[1]\", X_train_scaled.shape[1])\n",
    "\n",
    "# Define the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    # tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    # output layer should be 3 for the 3 classes\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', 'Precision', 'Recall'])\n",
    "\n",
    "# Train the model\n",
    "# the batch size refers to the number of training examples utilized in one iteration\n",
    "# in terms of accelerometers, this is the number of rows of data that are used to train the model at once\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=64) \n",
    "# model.fit(X_train_scaled, y_train, epochs=10, batch_size=32)\n",
    "# Make predictions on the test set\n",
    "# use the \n",
    "# print(\"y_pred\", y_pred)\n",
    "\n",
    "#  classifier.fit(X_train, y_train)\n",
    "    # y_pred = classifier.predict(X_test)\n",
    "    # accuracy = accuracy_score(y_test, y_pred)\n",
    "    # report = classification_report(y_test, y_pred)\n",
    "y_pred_proba = model.predict(X_test_scaled)\n",
    "# we have 3 different classes, so we need to change the threshold to 0.33, 0.66, and 1.0\n",
    "# y_pred = (y_pred_proba > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "# y_pred = (y_pred_proba > 0.66).astype(int)  # Convert probabilities to binary predictions\n",
    "# y_pred = (y_pred_proba > 0.33).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "# make a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification via SVM: This function will classify the data using SVM. It will return the accuracy of the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as pl\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "# Shuffle the data\n",
    "final_data = final_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = final_data.drop('label', axis=1)\n",
    "y = final_data['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize an SVM classifier\n",
    "svm_clf = SVC()\n",
    "\n",
    "# Train the classifier\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report)\n",
    "\n",
    "# Additional scores\n",
    "precision = precision_score(y_test, y_pred,average='micro')\n",
    "recall = recall_score(y_test, y_pred,average='micro')\n",
    "f1 = f1_score(y_test, y_pred,average='micro')\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pca\n",
    "from sklearn.decomposition import PCA\n",
    "# Assume that feature_names is a list of your feature names in the same order as in your training data\n",
    "feature_names = ['feature1', 'feature2', 'feature3', ...]  # replace with your actual feature names\n",
    "\n",
    "classifiers = [\n",
    "    RandomForestClassifier(n_estimators=100),\n",
    "    SVC(),\n",
    "    KNeighborsClassifier(),\n",
    "    LogisticRegression()\n",
    "]\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)  # adjust number of components as needed\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "for classifier in classifiers:\n",
    "    # Train and evaluate model with PCA data\n",
    "    classifier.fit(X_train_pca, y_train)\n",
    "    y_pred = classifier.predict(X_test_pca)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(type(classifier).__name__)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(report)\n",
    "    print()\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted labels\")\n",
    "    plt.ylabel(\"True labels\")\n",
    "    plt.title(f\"Confusion Matrix - {type(classifier).__name__}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Save the model\n",
    "    joblib.dump(classifier, f'models/{type(classifier).__name__}.pkl')\n",
    "\n",
    "    # Feature importance rating (only applicable for RandomForestClassifier)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification via a convolutional neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming 'features' is your DataFrame and 'labels' is your target\n",
    "X = features.values.reshape(-1, features.shape[1], 1)  # reshaping for CNN\n",
    "y = final_data['label'] # Use 'label' as the target\n",
    "y = pd.DataFrame(y).T\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, 2, activation='relu', input_shape=X_train[0].shape))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot IMU data\n",
    "plot_data(imu_df, 'IMU Data')\n",
    "\n",
    "# Plot accelerometer data\n",
    "plot_data(new_dataframe, 'Accelerometer Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new idea: use a LSTM RNN to use each of the sets of data to correctly train a lstm neural network to understand the sequences of data that we are getting from our data.\n",
    "### pull some code from INL stuff. \n",
    "## use the data from the imu and accel data to train the lstm neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_features(df, labels):\n",
    "#     # Convert payload lists into a DataFrame\n",
    "#     features = ['thumb_x', 'thumb_y', 'thumb_z', 'index_x', 'index_y', 'index_z', \n",
    "#             'middle_x', 'middle_y', 'middle_z', 'ring_x', 'ring_y', 'ring_z', \n",
    "#             'pinky_x', 'pinky_y', 'pinky_z']\n",
    "#     accel_df = pd.DataFrame(df[features])\n",
    "#     # Add labels to the DataFrame\n",
    "#     # get hte label from the df\n",
    "#     accel_df['label'] = labels\n",
    "# #     print(accel_df['label'])\n",
    "\n",
    "#     # Feature extraction\n",
    "#     avg_accel = accel_df.groupby('label').mean()\n",
    "#     std_dev_accel = accel_df.groupby('label').std()\n",
    "#     avg_abs_diff_accel = accel_df.groupby('label').apply(lambda x: x.diff().abs().mean())\n",
    "#     avg_accel_mag = accel_df.groupby('label').apply(lambda x: ((x**2).sum(axis=1)**0.5).mean())\n",
    "\n",
    "# #     time_between_peaks = accel_df.groupby('label').apply(lambda x: x.apply(lambda y: np.mean(np.diff(np.where((np.diff(y > 0) & (y > np.mean(y))))))))\n",
    "#     binned_distribution = accel_df.groupby('label').apply(lambda x: (np.histogram(x, bins=10)[0]).tolist())\n",
    "\n",
    "#     # Combine all features into a single DataFrame\n",
    "#     feature_data = pd.concat([avg_accel, \n",
    "#                               std_dev_accel, \n",
    "#                               avg_abs_diff_accel, \n",
    "#                               avg_accel_mag, \n",
    "#                         #       time_between_peaks,\n",
    "#                               binned_distribution], axis=1)\n",
    "#     return feature_data\n",
    "\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# # show one entry of data_df\n",
    "# # print(data_df)\n",
    "# feature_df = extract_features(data_df, labels)\n",
    "# # print(feature_df)\n",
    "\n",
    "# # 'feature_df' is your DataFrame of features and 'label' is your target\n",
    "# # X = feature_df.drop('label', axis=1)  # Use all columns except 'label' as features\n",
    "# X = feature_df  # Use all columns except 'label' as features\n",
    "# # print(feature_df[0])\n",
    "# y = labels # Use 'label' as the target\n",
    "# print(X)\n",
    "\n",
    "# # Split the data into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Initialize a Random Forest classifier\n",
    "# rf_clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# # Train the classifier\n",
    "# rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# # Evaluate the classifier\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def load_labeled_data(file_name):\n",
    "#     data = []\n",
    "#     with open(file_name, 'r') as f:\n",
    "#         for line in f:\n",
    "#             data.append(json.loads(line))\n",
    "    \n",
    "#     df = pd.DataFrame(data)\n",
    "#     # df = pd.DataFrame(data)\n",
    "\n",
    "#     # Break payload into separate columns\n",
    "#     df[['thumb_x', 'thumb_y', 'thumb_z', 'index_x', 'index_y', 'index_z', 'middle_x', 'middle_y', 'middle_z', \n",
    "#        'ring_x', 'ring_y', 'ring_z', 'pinky_x', 'pinky_y', 'pinky_z']] = pd.DataFrame(df['payload'].values.tolist(), index=df.index)\n",
    "\n",
    "#     # Drop the original 'payload' column\n",
    "#     df = df.drop(columns=['payload'])\n",
    "\n",
    "#     # Separate features from labels\n",
    "#     X = df.drop(columns=['label'])\n",
    "#     y = df['label']\n",
    "\n",
    "#     return X, y\n",
    "\n",
    "# # Loading data\n",
    "# X, y = load_labeled_data('../../data/Merged.json')\n",
    "# # print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import pandas\n",
    "# import pandas as pd\n",
    "\n",
    "# # concatenate all the features \n",
    "# concatenated_features = pd.DataFrame.from_dict(avg_accel, orient='index', columns=['avg_accel'])\n",
    "# concatenated_features = pd.concat([ #std_dev_accel,\n",
    "#                                    # avg_abs_diff_accel_df,\n",
    "#                                      avg_accel_mag_df, \n",
    "#                                       time_between_peaks_df, \n",
    "#                                    #   binned_distribution_df\n",
    "#                                       ], axis=1)\n",
    "# print(concatenated_features.head())\n",
    "# Combine all features into a single DataFrame\n",
    "# feature_data = pd.concat([\n",
    "#     avg_accel, \n",
    "#     # std_dev_accel, \n",
    "#     # avg_abs_diff_accel_df, \n",
    "#     # avg_accel_mag_df, \n",
    "#     # time_between_peaks_df,\n",
    "#     # binned_distribution_df\n",
    "# ], axis=1)\n",
    "# print(\"Feature Data\\n\", feature_data[0])\n",
    "# panda_avg_accel = pd.DataFrame(avg_accel)\n",
    "# type(panda_avg_accel)\n",
    "# print(panda_avg_accel)\n",
    "# p\n",
    "# Check the combined DataFrame\n",
    "# print(feature_data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
